{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "作者分類.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jkhnn/Novelist-Classification/blob/master/%E4%BD%9C%E8%80%85%E5%88%86%E9%A1%9E.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DTyHlQrvdRKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#参考・引用\n",
        "\n",
        "# 青空文庫で作者っぽさ判定（KERAS　＋　character-level cnn）　\n",
        "# https://qiita.com/cvusk/items/c1342dd0fff16dc37ddf\n",
        "  \n",
        "# character-level CNNでクリスマスを生き抜く\n",
        "# https://qiita.com/bokeneko/items/c0f0ce60a998304400c8\n",
        "  \n",
        "# Character-level Convolutional Networks for Text Classification\n",
        "# https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OulmPlSzP0rF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 学習・教師・テストデータデータダウンロード\n",
        "!git clone https://github.com/jkhnn/Novelist-Classification.git\n",
        "  \n",
        "# 前処理用モジュールのインストール\n",
        "!pip install neologdn\n",
        "\n",
        "# install MeCab\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "\n",
        "# gensim\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-zlopHd5hLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import neologdn\n",
        "import glob\n",
        "import os\n",
        "import MeCab\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Convolution2D, MaxPooling2D, Reshape, Input, merge\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, Callback, CSVLogger, ModelCheckpoint\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(font='Kozuka Gothic Pro', style=\"whitegrid\")\n",
        "aozora_dir = \"./\"\n",
        "\n",
        "\n",
        "def show_histgram(labels):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.set_title('samples num by authors')\n",
        "  ax.set_ylabel('samples num')\n",
        "  sns.distplot(labels, kde=False, rug=False, bins=25, axlabel=\"height\")\n",
        "  \n",
        "\n",
        "def show_similarity(model, authors):\n",
        "    data = {}\n",
        "    for a in authors:\n",
        "      data[a] = model.docvecs.most_similar([a])\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "def load_data(txt, max_length=200):\n",
        "    txt_list = []\n",
        "    for l in txt:\n",
        "        txt_line = [ord(x) for x in str(l).strip()]\n",
        "        # You will get encoded text in array, just like this\n",
        "        # [25991, 31456, 12391, 12399, 12394, 12367, 12387, \n",
        "        #12390, 23383, 24341, 12391, 12354, 12427, 12290]\n",
        "        txt_line = txt_line[:max_length]\n",
        "        txt_len = len(txt_line)\n",
        "        if txt_len < max_length:\n",
        "            txt_line += ([0] * (max_length - txt_len))\n",
        "        txt_list.append((txt_line))\n",
        "    return txt_list\n",
        "  \n",
        "  \n",
        "def create_model(dense_dim=9, dense_count=3, dropout=0.5, embed_size=128, max_length=200, filter_sizes=(2, 3, 4, 5), filter_num=64):\n",
        "    inp = Input(shape=(max_length,))\n",
        "    emb = Embedding(0xffff, embed_size)(inp)\n",
        "    emb_ex = Reshape((max_length, embed_size, 1))(emb)\n",
        "    convs = []\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = Convolution2D(filter_num, filter_size, embed_size, activation=\"relu\")(emb_ex)\n",
        "        pool = MaxPooling2D(pool_size=(max_length - filter_size + 1, 1))(conv)\n",
        "        convs.append(pool)\n",
        "    convs_merged = merge(convs, mode='concat')\n",
        "    obj = Reshape((filter_num * len(filter_sizes),))(convs_merged)\n",
        "    i = 0\n",
        "    while i < dense_count:\n",
        "      obj = Dense(64, activation=\"relu\")(obj)\n",
        "      obj = BatchNormalization()(obj)\n",
        "      obj = Dropout(dropout)(obj)\n",
        "      i += 1\n",
        "    fc3 = Dense(dense_dim, activation=\"sigmoid\")(obj)\n",
        "    model = Model(input=inp, output=fc3)\n",
        "    return model\n",
        "\n",
        "  \n",
        "def train(inputs, targets, filter_num=64, dense_count=3, dropout=0.5, filter_sizes=(2, 3, 4, 5), dense_dim=9, batch_size=100, epoch_count=10, \n",
        "    max_length=200, model_filepath=aozora_dir + \"model.h5\", learning_rate=0.001):\n",
        "  \n",
        "    start = learning_rate\n",
        "    stop = learning_rate * 0.01\n",
        "    learning_rates = np.linspace(start, stop, epoch_count)\n",
        "\n",
        "    model = create_model(max_length=max_length, filter_num=filter_num, dense_count=dense_count, dense_dim=dense_dim, filter_sizes=filter_sizes, dropout=dropout)\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Logging file for each epoch\n",
        "    csv_logger_file = '/tmp/clcnn_training.log'\n",
        "\n",
        "    # Checkpoint model for each epoch\n",
        "    checkpoint_filepath = \"/tmp/weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
        "\n",
        "    model.fit(inputs, targets,\n",
        "              nb_epoch=epoch_count,\n",
        "              batch_size=batch_size,\n",
        "              verbose=1,\n",
        "              validation_split=0.1,\n",
        "              shuffle=True,\n",
        "              callbacks=[\n",
        "                  LearningRateScheduler(lambda epoch: learning_rates[epoch]),\n",
        "                  CSVLogger(csv_logger_file),\n",
        "                  ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, \n",
        "                                  save_best_only=True, save_weights_only=False, monitor='val_acc')\n",
        "              ])\n",
        "    model.save(model_filepath)\n",
        "    return model\n",
        "      \n",
        "def show_training_and_validation_loss():\n",
        "    dataset1 = pd.read_csv(\"/tmp/clcnn_training.log\")\n",
        "    plt.clf()\n",
        "    plt.plot(dataset1[\"epoch\"], dataset1[\"loss\"], 'bo', label=\"Training loss\")\n",
        "    plt.plot(dataset1[\"epoch\"], dataset1[\"val_loss\"], 'b', label=\"Validation loss\")\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def show_training_and_validation_accuracy():\n",
        "    plt.clf()\n",
        "    dataset1 = pd.read_csv(\"/tmp/clcnn_training.log\")\n",
        "    plt.plot(dataset1[\"epoch\"], dataset1[\"acc\"], 'bo', label=\"Training acc\")\n",
        "    plt.plot(dataset1[\"epoch\"], dataset1[\"val_acc\"], 'b', label=\"Validation acc\")\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def get_stop_words(docs, limit_len=1):\n",
        "  c = collections.Counter(docs)\n",
        "  return [x for x,y in c.most_common() if y > limit_len]\n",
        "\n",
        "\n",
        "def remove_stop_words(docs, limit_len=1):\n",
        "  stop_words = get_stop_words(docs)\n",
        "  set_ab = set(docs) - set(stop_words)\n",
        "  list_ab = list(set_ab)\n",
        "  return list_ab\n",
        "\n",
        "\n",
        "def get_lines_by_path(file_path, if_normalize=True, stop_words_frequency=None):\n",
        "  # テキストファイルの読み込み\n",
        "  lines = []\n",
        "  with open(file_path) as fd:\n",
        "      for line in fd:\n",
        "          val = line.rstrip()\n",
        "          if val is None or len(val) == 0:\n",
        "            continue\n",
        "          if if_normalize:\n",
        "            lines.append(neologdn.normalize(val))\n",
        "          else:\n",
        "            lines.append(val)\n",
        "  if stop_words_frequency:\n",
        "    lines = remove_stop_words(lines, limit_len=stop_words_frequency)\n",
        "  return lines\n",
        "\n",
        "\n",
        "def get_val_and_label(exclude_authors, train_data_size, if_normalize, stop_words_frequency=None, data_type=\"Train\"):\n",
        "  root_path = \"./Novelist-Classification/data/master.csv\"\n",
        "  docs = []\n",
        "  authors = []\n",
        "  values = []\n",
        "  labels = []\n",
        "  labels_authors = []\n",
        "  index = 0\n",
        "  with open(root_path) as fd:\n",
        "      for i, line in enumerate(fd):\n",
        "        vals = line.rstrip().split(\",\")\n",
        "        author = vals[1].replace('\"', '')\n",
        "        if exclude_authors and (author in exclude_authors):\n",
        "          continue\n",
        "        authors.append(author)\n",
        "        val_ = get_lines_by_path(\"./Novelist-Classification/data/\" + data_type + \"/\" + vals[0].replace('\"', ''), \n",
        "                                 if_normalize=if_normalize, stop_words_frequency=stop_words_frequency)\n",
        "        val_ = val_[:train_data_size]\n",
        "        values.extend(val_)\n",
        "        lb = np.full(len(val_), index)\n",
        "        labels.extend(lb)\n",
        "        lb_author = np.full(len(val_), author)\n",
        "        labels_authors.extend(lb_author)\n",
        "        index += 1\n",
        "  return values, labels, labels_authors\n",
        "\n",
        "def execute_train_and_validation(exclude_authors, filter_num=64, dense_count=3, dense_dim=9, dropout=0.5, filter_sizes=(2, 3, 4, 5), epoch_count=10, batch_size=100,\n",
        "                  train_data_size=10000000, max_length=200, if_normalize=True, \n",
        "                  learning_rate=0.001, stop_words_frequency=1):\n",
        "\n",
        "\n",
        "  values_train, labels_train, _ = get_val_and_label(exclude_authors, train_data_size, if_normalize, stop_words_frequency, data_type=\"Train\")\n",
        "  values_test, labels_test, _ = get_val_and_label(exclude_authors, train_data_size, if_normalize, stop_words_frequency, data_type=\"Test\")\n",
        "  \n",
        "  x_train = load_data(values_train, max_length=max_length)\n",
        "  y_train = np_utils.to_categorical(labels_train)\n",
        "  \n",
        "  x_test = load_data(values_test, max_length=max_length)\n",
        "  y_test = np_utils.to_categorical(labels_test)\n",
        "  \n",
        "  model = train(np.array(x_train), y_train, filter_num=filter_num, dense_count=dense_count, dropout=dropout, filter_sizes=filter_sizes, learning_rate=learning_rate, \n",
        "        epoch_count=epoch_count, dense_dim=(dense_dim - len(exclude_authors)),  batch_size=batch_size, max_length=max_length)\n",
        "  \n",
        "  show_training_and_validation_loss()\n",
        "  show_training_and_validation_accuracy()\n",
        "  score = model.evaluate(np.array(x_test),y_test, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kn3YKUtfmun6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#作者一覧\n",
        "!head -300 Novelist-Classification/data/master.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fza4fUjGcJs3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0001.txt\",\"太宰治\"\n",
        "!head -50 Novelist-Classification/data/Train/0001.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2BGevTScF0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0002.txt\",\"森鴎外\"\n",
        "!head -300 Novelist-Classification/data/Train/0002.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H0R_yFA2ds_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0003.txt\",\"坂口安吾\"\n",
        "!head -300 Novelist-Classification/data/Train/0003.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0JA9kileAWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0004.txt\",\"夏目漱石\"\n",
        "!head -300 Novelist-Classification/data/Train/0004.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uk6rkShTeFh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0005.txt\",\"宮沢賢治\"\n",
        "!head -300 Novelist-Classification/data/Train/0005.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBAjJq_keKwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0006.txt\",\"与謝野晶子\"\n",
        "!head -300 Novelist-Classification/data/Train/0006.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlL9mz7FeQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0007.txt\",\"梶井基次郎\"\n",
        "!head -300 Novelist-Classification/data/Train/0007.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MXQh2VoeUj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"0008.txt\",\"芥川龍之介\"\n",
        "!head -300 Novelist-Classification/data/Train/0008.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xKDthizsmsv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"0009.txt\",\"北大路魯山人\"\n",
        "!head -300 Novelist-Classification/data/Train/0009.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5X3gUAzZ7F7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#doc2vecを使って作者同士の類似性を見る\n",
        "root_path = \"./Novelist-Classification/data/master.csv\"\n",
        "docs = []\n",
        "authors = []\n",
        "texts = []\n",
        "with open(root_path) as fd:\n",
        "    for i, line in enumerate(fd):\n",
        "      vals = line.rstrip().split(\",\")\n",
        "      author = vals[1].replace('\"', '')\n",
        "      val = get_lines_by_path(\"./Novelist-Classification/data/Train/\" + vals[0].replace('\"', ''))\n",
        "      authors.append(author)\n",
        "      docs.append(TaggedDocument(words=val, tags=[author]))\n",
        "      texts.append(val)\n",
        "    \n",
        "model = Doc2Vec(documents=docs, dm = 1, vector_size=200, window=8, min_count=1)\n",
        "show_similarity(model, authors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJrcQApZdrbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ラベルごとのサンプル数を見る（最初にやるべきが、、）\n",
        "exclude_authors = None\n",
        "values_train, labels_train, labels_authors_train  = get_val_and_label(exclude_authors, train_data_size=100000, if_normalize=True, stop_words_frequency=1, data_type=\"Train\")\n",
        "values_test, labels_test, labels_authors_test  = get_val_and_label(exclude_authors, train_data_size=100000, if_normalize=True, stop_words_frequency=1, data_type=\"Test\")\n",
        "show_histgram(labels_train)\n",
        "show_histgram(labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MEK1otNaSWa8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "execute_train_and_validation(exclude_authors=[\"与謝野晶子\", \"森鴎外\", \"宮沢賢治\"], dropout=0.2, filter_num=200, dense_count=1, learning_rate=0.001, epoch_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZmYmuGNBzDIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "execute_train_and_validation(exclude_authors=[\"与謝野晶子\", \"森鴎外\", \"宮沢賢治\"], dropout=0.2, filter_num=256, dense_count=2, learning_rate=0.001, epoch_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0FM6ZRZJhZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "execute_train_and_validation(exclude_authors=[\"与謝野晶子\", \"森鴎外\", \"宮沢賢治\"], dropout=0.2, filter_num=256, dense_count=1, learning_rate=0.001, epoch_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ElvepoDnE7Z_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "execute_train_and_validation(exclude_authors=[\"与謝野晶子\", \"森鴎外\", \"宮沢賢治\"], dropout=0.2, filter_num=128, dense_count=1, learning_rate=0.001, epoch_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPHCQsGdpiPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "execute_train_and_validation(exclude_authors=[\"与謝野晶子\", \"森鴎外\", \"宮沢賢治\"], dropout=0.2, filter_num=64, dense_count=2, learning_rate=0.001, epoch_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3QPG7zE3T80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}